{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Transfer Learning technique to do images classification ##\n",
    "# \"\"\"\n",
    "# Workaround Notes:\n",
    "# 1. Q: the validation_split argument in ImageDataGenerator not supported in Keras 2.1.3(server version)\n",
    "#    A: upgrade to the latest Keras(version 2.2.2): pip install keras --upgrade\n",
    "# 2. Q: Activation \"softmax\" in the latest Keras(version 2.2.2) not matched TensorFlow 1.4(server version)\n",
    "#    A: change Activation \"softmax\" to tf.nn.softmax\n",
    "# => Keras 2.1.5 is exactly for tensorflow 1.4.1! Instead of using \"pip install keras==2.1.5\" to overcome both Q1&Q2.\n",
    "#\n",
    "# Experimental Result:\n",
    "# Keras 2.1.5 + tensorflow 1.4.1 got better accuracy than Keras 2.2.2 + tensorflow 1.4.1\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Model)          (None, 7, 7, 1024)        7037504   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               12845312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 19,885,125\n",
      "Trainable params: 12,847,109\n",
      "Non-trainable params: 7,038,016\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "\n",
    "TRAIN_IMG_DIR = \"./train/\" #training_set at ./train/\n",
    "TEST_IMG_DIR = \"./test/\" #testing_set at ./test/testimg/\n",
    "\n",
    "NUM_CLASSES = 5 #target labels(ground truth), total 5 classes(check mapping.txt)\n",
    "\n",
    "# Image shapes\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "INPUT_SHAPE = (IMG_WIDTH, IMG_HEIGHT, CHANNELS)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "\n",
    "## Loading pre-trained network models in Keras\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg19 import VGG19\n",
    "# from keras.applications.resnet50 import ResNet50\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "# from keras.applications.mobilenet import MobileNet\n",
    "# from keras.applications.xception import Xception\n",
    "from keras.applications.densenet import DenseNet121\n",
    "# from keras.applications.densenet import DenseNet169\n",
    "# from keras.applications.densenet import DenseNet201\n",
    "\n",
    "## Setting pre-trained network models\n",
    "DenseNet_model = DenseNet121(include_top = False, weights = \"imagenet\", input_shape = INPUT_SHAPE)\n",
    "conv_base = DenseNet_model\n",
    "\n",
    "## Create our model based on the pre-trained network model\n",
    "model = Sequential()\n",
    "model.add(conv_base) #comes from the pre-trained model\n",
    "\n",
    "#Fully-connected NN layers\n",
    "#fully-connected 1st layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#fully-connected final layer\n",
    "model.add(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "# !change Activation from keras to tf.nn.softmax, because TF version too old on Server!\n",
    "# model.add(Dense(NUM_CLASSES))\n",
    "# import tensorflow as tf\n",
    "# model.add(Activation(tf.nn.softmax))\n",
    "\n",
    "# Freeze the base model before model.compile\n",
    "# conv_base.trainable = False #not work well. Why?\n",
    "for layer in conv_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# opt_adam = optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "# opt_rmsprop = optimizers.RMSprop(lr=1e-5, decay=0.01)\n",
    "model.compile(loss = \"categorical_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3062 images belonging to 5 classes.\n",
      "Found 761 images belonging to 5 classes.\n",
      "Found 500 images belonging to 1 classes.\n",
      "3062\n",
      "761\n",
      "500\n",
      "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}\n",
      "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "## Using Keras ImageDataGenerator to load images batch and do data augmentation on the fly.\n",
    "#!validation_split argument not supported in Keras 2.1.3(server version)!\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        rotation_range = 20,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        validation_split = 0.20\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "#         rotation_range = 20,\n",
    "#         width_shift_range = 0.2,\n",
    "#         height_shift_range = 0.2,\n",
    "#         shear_range = 0.2,\n",
    "#         zoom_range = 0.2,\n",
    "#         horizontal_flip = True,\n",
    "        validation_split = 0.20\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "## Using Keras datagen.flow_from_directory to load images from every sub-directories at train,(validation),test directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        directory = TRAIN_IMG_DIR,\n",
    "        target_size = (IMG_WIDTH, IMG_HEIGHT),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = BATCH_SIZE,\n",
    "        class_mode = \"categorical\",\n",
    "        shuffle = True,\n",
    "        seed = 33,\n",
    "        subset = \"training\"\n",
    ")\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        directory = TRAIN_IMG_DIR,\n",
    "        target_size = (IMG_WIDTH, IMG_HEIGHT),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = BATCH_SIZE,\n",
    "        class_mode = \"categorical\",\n",
    "        shuffle = True,\n",
    "        seed = 33,\n",
    "        subset = \"validation\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        directory = TEST_IMG_DIR,\n",
    "        target_size = (IMG_WIDTH, IMG_HEIGHT),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = 1,\n",
    "        class_mode = None,\n",
    "        shuffle = False\n",
    ")\n",
    "\n",
    "## Amounts of individual set: training, validation, test\n",
    "print (train_generator.n) #amounts of train_generator\n",
    "print (validation_generator.n) #amounts of validation_generator\n",
    "print (test_generator.n) #amounts of test_generator\n",
    "\n",
    "## Labels from Keras data generator\n",
    "print (train_generator.class_indices)\n",
    "print (validation_generator.class_indices)\n",
    "\n",
    "## Image shape check\n",
    "print (train_generator.image_shape)\n",
    "print (validation_generator.image_shape)\n",
    "print (test_generator.image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "191/191 [==============================] - 41s 214ms/step - loss: 0.9034 - acc: 0.7372 - val_loss: 0.6683 - val_acc: 0.7912\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79122, saving model to ./checkpoint-01-0.67-0.79.hdf5\n",
      "Epoch 2/100\n",
      "191/191 [==============================] - 38s 199ms/step - loss: 0.5257 - acc: 0.8233 - val_loss: 0.5324 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.79122 to 0.81649, saving model to ./checkpoint-02-0.53-0.82.hdf5\n",
      "Epoch 3/100\n",
      "191/191 [==============================] - 38s 200ms/step - loss: 0.4191 - acc: 0.8498 - val_loss: 0.5979 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/100\n",
      "191/191 [==============================] - 38s 198ms/step - loss: 0.4165 - acc: 0.8576 - val_loss: 0.5562 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.81649 to 0.82447, saving model to ./checkpoint-04-0.56-0.82.hdf5\n",
      "Epoch 5/100\n",
      "191/191 [==============================] - 38s 197ms/step - loss: 0.3797 - acc: 0.8609 - val_loss: 0.5217 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/100\n",
      "191/191 [==============================] - 38s 199ms/step - loss: 0.3749 - acc: 0.8631 - val_loss: 0.5219 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/100\n",
      "191/191 [==============================] - 37s 196ms/step - loss: 0.3346 - acc: 0.8794 - val_loss: 0.5252 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/100\n",
      "191/191 [==============================] - 38s 197ms/step - loss: 0.3504 - acc: 0.8758 - val_loss: 0.5401 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/100\n",
      "191/191 [==============================] - 37s 194ms/step - loss: 0.3235 - acc: 0.8825 - val_loss: 0.4619 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/100\n",
      "191/191 [==============================] - 37s 194ms/step - loss: 0.3236 - acc: 0.8881 - val_loss: 0.4962 - val_acc: 0.8404\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.82447 to 0.84043, saving model to ./checkpoint-10-0.50-0.84.hdf5\n",
      "Epoch 11/100\n",
      "191/191 [==============================] - 37s 193ms/step - loss: 0.3226 - acc: 0.8853 - val_loss: 0.4762 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 0.2878 - acc: 0.8997 - val_loss: 0.4479 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.84043 to 0.85372, saving model to ./checkpoint-12-0.45-0.85.hdf5\n",
      "Epoch 13/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2806 - acc: 0.8983 - val_loss: 0.5135 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/100\n",
      "191/191 [==============================] - 37s 194ms/step - loss: 0.2793 - acc: 0.8998 - val_loss: 0.5668 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 0.2859 - acc: 0.9012 - val_loss: 0.5278 - val_acc: 0.8285\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.2806 - acc: 0.8990 - val_loss: 0.5125 - val_acc: 0.8351\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2863 - acc: 0.8943 - val_loss: 0.5202 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/100\n",
      "191/191 [==============================] - 36s 187ms/step - loss: 0.2672 - acc: 0.9010 - val_loss: 0.5914 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/100\n",
      "191/191 [==============================] - 37s 191ms/step - loss: 0.2782 - acc: 0.9033 - val_loss: 0.5170 - val_acc: 0.8418\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/100\n",
      "191/191 [==============================] - 36s 188ms/step - loss: 0.2671 - acc: 0.9105 - val_loss: 0.4962 - val_acc: 0.8351\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 21/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 0.2519 - acc: 0.9051 - val_loss: 0.5076 - val_acc: 0.8338\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.2868 - acc: 0.8974 - val_loss: 0.5007 - val_acc: 0.8457\n",
      "\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 23/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2726 - acc: 0.9071 - val_loss: 0.5173 - val_acc: 0.8258\n",
      "\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 24/100\n",
      "191/191 [==============================] - 36s 188ms/step - loss: 0.2721 - acc: 0.9005 - val_loss: 0.4646 - val_acc: 0.8378\n",
      "\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 25/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2710 - acc: 0.9016 - val_loss: 0.4782 - val_acc: 0.8404\n",
      "\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 26/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.2515 - acc: 0.9093 - val_loss: 0.5526 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 27/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.2336 - acc: 0.9149 - val_loss: 0.5594 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 28/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 0.2584 - acc: 0.9015 - val_loss: 0.5945 - val_acc: 0.8285\n",
      "\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 29/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.2359 - acc: 0.9101 - val_loss: 0.5569 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 30/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.2510 - acc: 0.9120 - val_loss: 0.5074 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 31/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.2405 - acc: 0.9114 - val_loss: 0.5111 - val_acc: 0.8418\n",
      "\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 32/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.2405 - acc: 0.9120 - val_loss: 0.6005 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 33/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2013 - acc: 0.9241 - val_loss: 0.6065 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 34/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2125 - acc: 0.9140 - val_loss: 0.5612 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 35/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2140 - acc: 0.9235 - val_loss: 0.5359 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 36/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.2238 - acc: 0.9170 - val_loss: 0.4928 - val_acc: 0.8378\n",
      "\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 37/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2218 - acc: 0.9160 - val_loss: 0.4469 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 38/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2121 - acc: 0.9230 - val_loss: 0.5420 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00038: val_acc did not improve\n",
      "Epoch 39/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.2204 - acc: 0.9161 - val_loss: 0.4755 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.85372 to 0.85638, saving model to ./checkpoint-39-0.48-0.86.hdf5\n",
      "Epoch 40/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.2272 - acc: 0.9154 - val_loss: 0.4896 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00040: val_acc did not improve\n",
      "Epoch 41/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.2201 - acc: 0.9241 - val_loss: 0.5633 - val_acc: 0.8258\n",
      "\n",
      "Epoch 00041: val_acc did not improve\n",
      "Epoch 42/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2139 - acc: 0.9230 - val_loss: 0.5036 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00042: val_acc did not improve\n",
      "Epoch 43/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.2065 - acc: 0.9248 - val_loss: 0.5122 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00043: val_acc did not improve\n",
      "Epoch 44/100\n",
      " 15/191 [=>............................] - ETA: 11s - loss: 0.2175 - acc: 0.9167\n",
      "Epoch 00046: val_acc did not improve\n",
      "Epoch 47/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1944 - acc: 0.9270 - val_loss: 0.5929 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00047: val_acc did not improve\n",
      "Epoch 48/100\n",
      "191/191 [==============================] - 37s 191ms/step - loss: 0.1968 - acc: 0.9329 - val_loss: 0.6460 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00048: val_acc did not improve\n",
      "Epoch 49/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1892 - acc: 0.9281 - val_loss: 0.6345 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00049: val_acc did not improve\n",
      "Epoch 50/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.2154 - acc: 0.9275 - val_loss: 0.7242 - val_acc: 0.7846\n",
      "\n",
      "Epoch 00050: val_acc did not improve\n",
      "Epoch 51/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1941 - acc: 0.9303 - val_loss: 0.5804 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00051: val_acc did not improve\n",
      "Epoch 52/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.2104 - acc: 0.9250 - val_loss: 0.5825 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00052: val_acc did not improve\n",
      "Epoch 53/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.2111 - acc: 0.9211 - val_loss: 0.5777 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00053: val_acc did not improve\n",
      "Epoch 54/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.2178 - acc: 0.9245 - val_loss: 0.6104 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00054: val_acc did not improve\n",
      "Epoch 55/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.2086 - acc: 0.9239 - val_loss: 0.5785 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00056: val_acc did not improve\n",
      "Epoch 57/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 0.1913 - acc: 0.9257 - val_loss: 0.6132 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00057: val_acc did not improve\n",
      "Epoch 58/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1995 - acc: 0.9274 - val_loss: 0.5236 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00058: val_acc did not improve\n",
      "Epoch 59/100\n",
      "191/191 [==============================] - 37s 191ms/step - loss: 0.1642 - acc: 0.9440 - val_loss: 0.5564 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00059: val_acc did not improve\n",
      "Epoch 60/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1949 - acc: 0.9322 - val_loss: 0.6224 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00060: val_acc did not improve\n",
      "Epoch 61/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.2147 - acc: 0.9253 - val_loss: 0.5562 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00063: val_acc did not improve\n",
      "Epoch 64/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 0.1764 - acc: 0.9385 - val_loss: 0.5636 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00064: val_acc did not improve\n",
      "Epoch 65/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.1722 - acc: 0.9383 - val_loss: 0.5279 - val_acc: 0.8364\n",
      "\n",
      "Epoch 00065: val_acc did not improve\n",
      "Epoch 66/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.1699 - acc: 0.9382 - val_loss: 0.5845 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00066: val_acc did not improve\n",
      "Epoch 67/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1792 - acc: 0.9352 - val_loss: 0.5845 - val_acc: 0.8338\n",
      "\n",
      "Epoch 00067: val_acc did not improve\n",
      "Epoch 68/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.1460 - acc: 0.9512 - val_loss: 0.5796 - val_acc: 0.8351\n",
      "\n",
      "Epoch 00068: val_acc did not improve\n",
      "Epoch 69/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1855 - acc: 0.9304 - val_loss: 0.5341 - val_acc: 0.8457\n",
      "\n",
      "Epoch 00069: val_acc did not improve\n",
      "Epoch 70/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.1637 - acc: 0.9424 - val_loss: 0.5734 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00070: val_acc did not improve\n",
      "Epoch 71/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.1736 - acc: 0.9353 - val_loss: 0.5778 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00071: val_acc did not improve\n",
      "Epoch 72/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.1762 - acc: 0.9403 - val_loss: 0.5252 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00072: val_acc did not improve\n",
      "Epoch 73/100\n",
      "191/191 [==============================] - 37s 191ms/step - loss: 0.1637 - acc: 0.9414 - val_loss: 0.5744 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00073: val_acc did not improve\n",
      "Epoch 74/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.1691 - acc: 0.9331 - val_loss: 0.5780 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00074: val_acc did not improve\n",
      "Epoch 75/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.1973 - acc: 0.9289 - val_loss: 0.6232 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00075: val_acc did not improve\n",
      "Epoch 76/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1655 - acc: 0.9444 - val_loss: 0.5832 - val_acc: 0.8364\n",
      "\n",
      "Epoch 00076: val_acc did not improve\n",
      "Epoch 77/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 0.1613 - acc: 0.9450 - val_loss: 0.5316 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00077: val_acc did not improve\n",
      "Epoch 78/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1692 - acc: 0.9390 - val_loss: 0.5975 - val_acc: 0.8378\n",
      "\n",
      "Epoch 00078: val_acc did not improve\n",
      "Epoch 79/100\n",
      "191/191 [==============================] - 37s 191ms/step - loss: 0.1572 - acc: 0.9469 - val_loss: 0.5796 - val_acc: 0.8457\n",
      "\n",
      "Epoch 00079: val_acc did not improve\n",
      "Epoch 80/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 0.1573 - acc: 0.9449 - val_loss: 0.6826 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00080: val_acc did not improve\n",
      "Epoch 81/100\n",
      "191/191 [==============================] - 37s 193ms/step - loss: 0.1506 - acc: 0.9467 - val_loss: 0.6597 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00081: val_acc did not improve\n",
      "Epoch 82/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.1540 - acc: 0.9463 - val_loss: 0.6229 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00082: val_acc did not improve\n",
      "Epoch 83/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1572 - acc: 0.9408 - val_loss: 0.5632 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00083: val_acc did not improve\n",
      "Epoch 84/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 0.1718 - acc: 0.9391 - val_loss: 0.5767 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00084: val_acc did not improve\n",
      "Epoch 85/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.1699 - acc: 0.9439 - val_loss: 0.5761 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00085: val_acc did not improve\n",
      "Epoch 86/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.1613 - acc: 0.9480 - val_loss: 0.6296 - val_acc: 0.8351\n",
      "\n",
      "Epoch 00086: val_acc did not improve\n",
      "Epoch 87/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1718 - acc: 0.9428 - val_loss: 0.5819 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00087: val_acc did not improve\n",
      "Epoch 88/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.1476 - acc: 0.9470 - val_loss: 0.6072 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00088: val_acc did not improve\n",
      "Epoch 89/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.1681 - acc: 0.9439 - val_loss: 0.6404 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00089: val_acc did not improve\n",
      "Epoch 90/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.1430 - acc: 0.9499 - val_loss: 0.6439 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00090: val_acc did not improve\n",
      "Epoch 91/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.1545 - acc: 0.9460 - val_loss: 0.5949 - val_acc: 0.8444\n",
      "\n",
      "Epoch 00091: val_acc did not improve\n",
      "Epoch 92/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.1561 - acc: 0.9480 - val_loss: 0.6275 - val_acc: 0.8364\n",
      "\n",
      "Epoch 00092: val_acc did not improve\n",
      "Epoch 93/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.1499 - acc: 0.9484 - val_loss: 0.5763 - val_acc: 0.8351\n",
      "\n",
      "Epoch 00093: val_acc did not improve\n",
      "Epoch 94/100\n",
      "191/191 [==============================] - 37s 193ms/step - loss: 0.1607 - acc: 0.9454 - val_loss: 0.6254 - val_acc: 0.8285\n",
      "\n",
      "Epoch 00094: val_acc did not improve\n",
      "Epoch 95/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 0.1401 - acc: 0.9480 - val_loss: 0.6014 - val_acc: 0.8378\n",
      "\n",
      "Epoch 00095: val_acc did not improve\n",
      "Epoch 96/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.1591 - acc: 0.9425 - val_loss: 0.7298 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00096: val_acc did not improve\n",
      "Epoch 97/100\n",
      "191/191 [==============================] - 37s 191ms/step - loss: 0.1393 - acc: 0.9522 - val_loss: 0.6601 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00097: val_acc did not improve\n",
      "Epoch 98/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 0.1532 - acc: 0.9495 - val_loss: 0.6389 - val_acc: 0.8258\n",
      "\n",
      "Epoch 00098: val_acc did not improve\n",
      "Epoch 99/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.1505 - acc: 0.9506 - val_loss: 0.7145 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00099: val_acc did not improve\n",
      "Epoch 100/100\n",
      "191/191 [==============================] - 37s 191ms/step - loss: 0.1445 - acc: 0.9473 - val_loss: 0.7215 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00100: val_acc did not improve\n",
      "500/500 [==============================] - 11s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "## Fitting/Training the model\n",
    "STEPS_PER_EPOCH = train_generator.n // BATCH_SIZE\n",
    "VALIDATION_STEPS = validation_generator.n // BATCH_SIZE\n",
    "\n",
    "# Callbacks setting\n",
    "FILE_PATH = \"./checkpoint-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}.hdf5\"\n",
    "# EarlyStop = EarlyStopping(monitor=\"val_acc\", patience=50, verbose=1, mode=\"max\")\n",
    "Checkpoint = ModelCheckpoint(FILE_PATH, monitor=\"val_acc\", verbose=1, save_best_only=True, mode=\"max\")\n",
    "#Callback list with Earlystop\n",
    "# Callback_list = [EarlyStop, Checkpoint]\n",
    "#Callback list without Earlystop\n",
    "Callback_list = [Checkpoint]\n",
    "\n",
    "history = model.fit_generator(\n",
    "                generator = train_generator,\n",
    "                steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                epochs = EPOCHS,\n",
    "                callbacks = Callback_list,\n",
    "                validation_data = validation_generator,\n",
    "                validation_steps = VALIDATION_STEPS,\n",
    "                shuffle = True\n",
    ")\n",
    "\n",
    "## Evaluate the model\n",
    "# model.evaluate_generator(generator = )\n",
    "\n",
    "## Predict the test set, then we'll get a probability nparray\n",
    "test_generator.reset()\n",
    "pred_probability = model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.983479e-01</td>\n",
       "      <td>1.640795e-03</td>\n",
       "      <td>8.491694e-10</td>\n",
       "      <td>1.126433e-05</td>\n",
       "      <td>1.586786e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.717133e-01</td>\n",
       "      <td>1.031454e-05</td>\n",
       "      <td>4.445067e-10</td>\n",
       "      <td>2.282764e-01</td>\n",
       "      <td>8.408777e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.666517e-05</td>\n",
       "      <td>5.689669e-09</td>\n",
       "      <td>1.401951e-03</td>\n",
       "      <td>2.075839e-05</td>\n",
       "      <td>9.984806e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.763311e-04</td>\n",
       "      <td>2.415565e-07</td>\n",
       "      <td>4.734668e-13</td>\n",
       "      <td>9.997234e-01</td>\n",
       "      <td>2.545340e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.078311e-02</td>\n",
       "      <td>3.775751e-08</td>\n",
       "      <td>8.128084e-11</td>\n",
       "      <td>9.792169e-01</td>\n",
       "      <td>5.796841e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.029262e-06</td>\n",
       "      <td>3.057284e-11</td>\n",
       "      <td>9.999233e-01</td>\n",
       "      <td>1.079883e-10</td>\n",
       "      <td>7.562804e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>6.260333e-12</td>\n",
       "      <td>6.247728e-15</td>\n",
       "      <td>3.115984e-07</td>\n",
       "      <td>1.873542e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.999917e-01</td>\n",
       "      <td>3.034085e-06</td>\n",
       "      <td>4.592570e-08</td>\n",
       "      <td>5.281284e-06</td>\n",
       "      <td>7.158533e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.926997e-01</td>\n",
       "      <td>4.177305e-08</td>\n",
       "      <td>2.574224e-13</td>\n",
       "      <td>7.073002e-01</td>\n",
       "      <td>2.000232e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.959112e-05</td>\n",
       "      <td>3.610704e-08</td>\n",
       "      <td>6.379376e-13</td>\n",
       "      <td>9.999104e-01</td>\n",
       "      <td>2.534951e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.601681e-03</td>\n",
       "      <td>9.982203e-01</td>\n",
       "      <td>2.688400e-09</td>\n",
       "      <td>1.779895e-04</td>\n",
       "      <td>2.624818e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>1.234999e-09</td>\n",
       "      <td>4.178054e-19</td>\n",
       "      <td>1.357315e-07</td>\n",
       "      <td>1.012599e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.158921e-03</td>\n",
       "      <td>3.450554e-03</td>\n",
       "      <td>3.391951e-06</td>\n",
       "      <td>9.953848e-01</td>\n",
       "      <td>2.398080e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.987866e-03</td>\n",
       "      <td>9.969746e-01</td>\n",
       "      <td>4.485965e-13</td>\n",
       "      <td>1.037561e-03</td>\n",
       "      <td>8.485315e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.888944e-05</td>\n",
       "      <td>9.999269e-01</td>\n",
       "      <td>1.621516e-11</td>\n",
       "      <td>1.424153e-05</td>\n",
       "      <td>5.525038e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.955397e-01</td>\n",
       "      <td>1.685577e-05</td>\n",
       "      <td>1.707479e-03</td>\n",
       "      <td>2.735672e-03</td>\n",
       "      <td>1.828536e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.731878e-01</td>\n",
       "      <td>1.442994e-07</td>\n",
       "      <td>3.797591e-05</td>\n",
       "      <td>2.064631e-04</td>\n",
       "      <td>8.265676e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.563404e-06</td>\n",
       "      <td>1.320386e-05</td>\n",
       "      <td>1.323397e-01</td>\n",
       "      <td>1.245850e-04</td>\n",
       "      <td>8.675159e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.239049e-01</td>\n",
       "      <td>1.757775e-01</td>\n",
       "      <td>2.353965e-14</td>\n",
       "      <td>3.176372e-04</td>\n",
       "      <td>1.252537e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.844392e-08</td>\n",
       "      <td>9.999970e-01</td>\n",
       "      <td>1.019715e-13</td>\n",
       "      <td>2.963093e-06</td>\n",
       "      <td>5.950032e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.138698e-01</td>\n",
       "      <td>2.139808e-04</td>\n",
       "      <td>2.289135e-02</td>\n",
       "      <td>1.407310e-01</td>\n",
       "      <td>5.222939e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.870757e-02</td>\n",
       "      <td>5.255393e-03</td>\n",
       "      <td>3.361267e-01</td>\n",
       "      <td>4.316472e-01</td>\n",
       "      <td>1.682631e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.997924e-01</td>\n",
       "      <td>5.734399e-06</td>\n",
       "      <td>2.728064e-05</td>\n",
       "      <td>7.195096e-05</td>\n",
       "      <td>1.026737e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.331430e-07</td>\n",
       "      <td>5.829729e-12</td>\n",
       "      <td>6.129446e-10</td>\n",
       "      <td>1.458080e-07</td>\n",
       "      <td>9.999992e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.044536e-05</td>\n",
       "      <td>1.063271e-07</td>\n",
       "      <td>2.397160e-08</td>\n",
       "      <td>6.403083e-07</td>\n",
       "      <td>9.999487e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.593320e-04</td>\n",
       "      <td>8.852925e-10</td>\n",
       "      <td>6.839513e-09</td>\n",
       "      <td>5.046329e-08</td>\n",
       "      <td>9.994406e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.472409e-05</td>\n",
       "      <td>1.392585e-05</td>\n",
       "      <td>1.916719e-07</td>\n",
       "      <td>2.584871e-07</td>\n",
       "      <td>9.999208e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.195991e-04</td>\n",
       "      <td>4.123026e-08</td>\n",
       "      <td>4.658676e-06</td>\n",
       "      <td>7.691904e-06</td>\n",
       "      <td>9.996680e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.061405e-04</td>\n",
       "      <td>9.221442e-11</td>\n",
       "      <td>2.688411e-07</td>\n",
       "      <td>1.807893e-07</td>\n",
       "      <td>9.996934e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.386988e-08</td>\n",
       "      <td>2.816457e-12</td>\n",
       "      <td>5.618474e-12</td>\n",
       "      <td>2.995404e-07</td>\n",
       "      <td>9.999996e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2.436096e-04</td>\n",
       "      <td>2.737935e-05</td>\n",
       "      <td>7.438005e-06</td>\n",
       "      <td>8.857287e-06</td>\n",
       "      <td>9.997126e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>9.702116e-01</td>\n",
       "      <td>2.711919e-06</td>\n",
       "      <td>1.169868e-07</td>\n",
       "      <td>2.902211e-02</td>\n",
       "      <td>7.634361e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2.210524e-02</td>\n",
       "      <td>3.504625e-01</td>\n",
       "      <td>8.946024e-05</td>\n",
       "      <td>6.252848e-01</td>\n",
       "      <td>2.058015e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>1.637603e-06</td>\n",
       "      <td>9.999981e-01</td>\n",
       "      <td>1.756853e-08</td>\n",
       "      <td>1.214546e-08</td>\n",
       "      <td>2.379797e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>9.999982e-01</td>\n",
       "      <td>6.421461e-08</td>\n",
       "      <td>9.021034e-10</td>\n",
       "      <td>1.673768e-06</td>\n",
       "      <td>1.320929e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>3.193058e-02</td>\n",
       "      <td>1.111129e-01</td>\n",
       "      <td>5.279178e-06</td>\n",
       "      <td>1.031315e-02</td>\n",
       "      <td>8.466380e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>3.511230e-04</td>\n",
       "      <td>7.418345e-05</td>\n",
       "      <td>4.475200e-10</td>\n",
       "      <td>9.995747e-01</td>\n",
       "      <td>4.177504e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>9.620880e-01</td>\n",
       "      <td>2.303129e-02</td>\n",
       "      <td>3.165591e-10</td>\n",
       "      <td>1.488062e-02</td>\n",
       "      <td>1.070554e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>9.593784e-01</td>\n",
       "      <td>1.446757e-02</td>\n",
       "      <td>2.233359e-07</td>\n",
       "      <td>2.403128e-02</td>\n",
       "      <td>2.122510e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>9.941590e-01</td>\n",
       "      <td>8.056743e-04</td>\n",
       "      <td>7.282061e-11</td>\n",
       "      <td>5.035335e-03</td>\n",
       "      <td>4.475466e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>7.591950e-01</td>\n",
       "      <td>4.227752e-02</td>\n",
       "      <td>3.068105e-06</td>\n",
       "      <td>1.985217e-01</td>\n",
       "      <td>2.673788e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>7.792112e-04</td>\n",
       "      <td>7.118313e-05</td>\n",
       "      <td>1.329844e-01</td>\n",
       "      <td>1.056845e-05</td>\n",
       "      <td>8.661546e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>7.739377e-01</td>\n",
       "      <td>2.260261e-01</td>\n",
       "      <td>3.314419e-07</td>\n",
       "      <td>3.585913e-05</td>\n",
       "      <td>6.966411e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1.092750e-02</td>\n",
       "      <td>2.299382e-05</td>\n",
       "      <td>8.317283e-12</td>\n",
       "      <td>9.890496e-01</td>\n",
       "      <td>5.782632e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1.050001e-02</td>\n",
       "      <td>3.108550e-04</td>\n",
       "      <td>2.872357e-10</td>\n",
       "      <td>9.891891e-01</td>\n",
       "      <td>1.980020e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.314313e-01</td>\n",
       "      <td>2.310674e-02</td>\n",
       "      <td>3.168134e-08</td>\n",
       "      <td>6.447440e-01</td>\n",
       "      <td>7.179200e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.722488e-01</td>\n",
       "      <td>3.162927e-06</td>\n",
       "      <td>1.789180e-03</td>\n",
       "      <td>2.770401e-04</td>\n",
       "      <td>4.256818e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>8.732340e-02</td>\n",
       "      <td>8.555764e-05</td>\n",
       "      <td>9.720798e-07</td>\n",
       "      <td>9.125570e-01</td>\n",
       "      <td>3.303300e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1.885083e-03</td>\n",
       "      <td>5.174238e-05</td>\n",
       "      <td>9.971628e-01</td>\n",
       "      <td>3.972676e-04</td>\n",
       "      <td>5.031348e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>3.999638e-05</td>\n",
       "      <td>2.897862e-09</td>\n",
       "      <td>9.936967e-01</td>\n",
       "      <td>5.152752e-09</td>\n",
       "      <td>6.263216e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>9.933158e-01</td>\n",
       "      <td>6.605062e-03</td>\n",
       "      <td>1.655737e-06</td>\n",
       "      <td>7.180545e-05</td>\n",
       "      <td>5.784145e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>5.065328e-03</td>\n",
       "      <td>4.619337e-08</td>\n",
       "      <td>9.635092e-01</td>\n",
       "      <td>7.392908e-05</td>\n",
       "      <td>3.135137e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>3.646384e-02</td>\n",
       "      <td>9.606531e-01</td>\n",
       "      <td>2.318081e-09</td>\n",
       "      <td>2.883147e-03</td>\n",
       "      <td>3.232211e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>7.427807e-04</td>\n",
       "      <td>7.620057e-06</td>\n",
       "      <td>9.961189e-01</td>\n",
       "      <td>2.710964e-06</td>\n",
       "      <td>3.127969e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>1.078909e-06</td>\n",
       "      <td>2.618394e-06</td>\n",
       "      <td>2.894435e-07</td>\n",
       "      <td>9.876021e-07</td>\n",
       "      <td>9.999950e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>7.578716e-03</td>\n",
       "      <td>5.161685e-07</td>\n",
       "      <td>5.507310e-08</td>\n",
       "      <td>9.924194e-01</td>\n",
       "      <td>1.274363e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1.759027e-01</td>\n",
       "      <td>4.834744e-01</td>\n",
       "      <td>9.417763e-02</td>\n",
       "      <td>2.837596e-04</td>\n",
       "      <td>2.461615e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>5.554980e-06</td>\n",
       "      <td>9.999943e-01</td>\n",
       "      <td>1.626571e-13</td>\n",
       "      <td>1.028339e-07</td>\n",
       "      <td>1.953231e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>9.999973e-01</td>\n",
       "      <td>2.627073e-09</td>\n",
       "      <td>7.970892e-13</td>\n",
       "      <td>2.721571e-06</td>\n",
       "      <td>5.618244e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2.377645e-02</td>\n",
       "      <td>4.428895e-06</td>\n",
       "      <td>1.152405e-03</td>\n",
       "      <td>4.446667e-04</td>\n",
       "      <td>9.746221e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4\n",
       "0    9.983479e-01  1.640795e-03  8.491694e-10  1.126433e-05  1.586786e-11\n",
       "1    7.717133e-01  1.031454e-05  4.445067e-10  2.282764e-01  8.408777e-11\n",
       "2    9.666517e-05  5.689669e-09  1.401951e-03  2.075839e-05  9.984806e-01\n",
       "3    2.763311e-04  2.415565e-07  4.734668e-13  9.997234e-01  2.545340e-13\n",
       "4    2.078311e-02  3.775751e-08  8.128084e-11  9.792169e-01  5.796841e-10\n",
       "5    1.029262e-06  3.057284e-11  9.999233e-01  1.079883e-10  7.562804e-05\n",
       "6    9.999996e-01  6.260333e-12  6.247728e-15  3.115984e-07  1.873542e-14\n",
       "7    9.999917e-01  3.034085e-06  4.592570e-08  5.281284e-06  7.158533e-11\n",
       "8    2.926997e-01  4.177305e-08  2.574224e-13  7.073002e-01  2.000232e-13\n",
       "9    8.959112e-05  3.610704e-08  6.379376e-13  9.999104e-01  2.534951e-14\n",
       "10   1.601681e-03  9.982203e-01  2.688400e-09  1.779895e-04  2.624818e-08\n",
       "11   9.999999e-01  1.234999e-09  4.178054e-19  1.357315e-07  1.012599e-13\n",
       "12   1.158921e-03  3.450554e-03  3.391951e-06  9.953848e-01  2.398080e-06\n",
       "13   1.987866e-03  9.969746e-01  4.485965e-13  1.037561e-03  8.485315e-11\n",
       "14   5.888944e-05  9.999269e-01  1.621516e-11  1.424153e-05  5.525038e-09\n",
       "15   9.955397e-01  1.685577e-05  1.707479e-03  2.735672e-03  1.828536e-07\n",
       "16   1.731878e-01  1.442994e-07  3.797591e-05  2.064631e-04  8.265676e-01\n",
       "17   6.563404e-06  1.320386e-05  1.323397e-01  1.245850e-04  8.675159e-01\n",
       "18   8.239049e-01  1.757775e-01  2.353965e-14  3.176372e-04  1.252537e-10\n",
       "19   5.844392e-08  9.999970e-01  1.019715e-13  2.963093e-06  5.950032e-11\n",
       "20   3.138698e-01  2.139808e-04  2.289135e-02  1.407310e-01  5.222939e-01\n",
       "21   5.870757e-02  5.255393e-03  3.361267e-01  4.316472e-01  1.682631e-01\n",
       "22   9.997924e-01  5.734399e-06  2.728064e-05  7.195096e-05  1.026737e-04\n",
       "23   6.331430e-07  5.829729e-12  6.129446e-10  1.458080e-07  9.999992e-01\n",
       "24   5.044536e-05  1.063271e-07  2.397160e-08  6.403083e-07  9.999487e-01\n",
       "25   5.593320e-04  8.852925e-10  6.839513e-09  5.046329e-08  9.994406e-01\n",
       "26   6.472409e-05  1.392585e-05  1.916719e-07  2.584871e-07  9.999208e-01\n",
       "27   3.195991e-04  4.123026e-08  4.658676e-06  7.691904e-06  9.996680e-01\n",
       "28   3.061405e-04  9.221442e-11  2.688411e-07  1.807893e-07  9.996934e-01\n",
       "29   6.386988e-08  2.816457e-12  5.618474e-12  2.995404e-07  9.999996e-01\n",
       "..            ...           ...           ...           ...           ...\n",
       "470  2.436096e-04  2.737935e-05  7.438005e-06  8.857287e-06  9.997126e-01\n",
       "471  9.702116e-01  2.711919e-06  1.169868e-07  2.902211e-02  7.634361e-04\n",
       "472  2.210524e-02  3.504625e-01  8.946024e-05  6.252848e-01  2.058015e-03\n",
       "473  1.637603e-06  9.999981e-01  1.756853e-08  1.214546e-08  2.379797e-07\n",
       "474  9.999982e-01  6.421461e-08  9.021034e-10  1.673768e-06  1.320929e-09\n",
       "475  3.193058e-02  1.111129e-01  5.279178e-06  1.031315e-02  8.466380e-01\n",
       "476  3.511230e-04  7.418345e-05  4.475200e-10  9.995747e-01  4.177504e-08\n",
       "477  9.620880e-01  2.303129e-02  3.165591e-10  1.488062e-02  1.070554e-09\n",
       "478  9.593784e-01  1.446757e-02  2.233359e-07  2.403128e-02  2.122510e-03\n",
       "479  9.941590e-01  8.056743e-04  7.282061e-11  5.035335e-03  4.475466e-12\n",
       "480  7.591950e-01  4.227752e-02  3.068105e-06  1.985217e-01  2.673788e-06\n",
       "481  7.792112e-04  7.118313e-05  1.329844e-01  1.056845e-05  8.661546e-01\n",
       "482  7.739377e-01  2.260261e-01  3.314419e-07  3.585913e-05  6.966411e-09\n",
       "483  1.092750e-02  2.299382e-05  8.317283e-12  9.890496e-01  5.782632e-10\n",
       "484  1.050001e-02  3.108550e-04  2.872357e-10  9.891891e-01  1.980020e-09\n",
       "485  3.314313e-01  2.310674e-02  3.168134e-08  6.447440e-01  7.179200e-04\n",
       "486  5.722488e-01  3.162927e-06  1.789180e-03  2.770401e-04  4.256818e-01\n",
       "487  8.732340e-02  8.555764e-05  9.720798e-07  9.125570e-01  3.303300e-05\n",
       "488  1.885083e-03  5.174238e-05  9.971628e-01  3.972676e-04  5.031348e-04\n",
       "489  3.999638e-05  2.897862e-09  9.936967e-01  5.152752e-09  6.263216e-03\n",
       "490  9.933158e-01  6.605062e-03  1.655737e-06  7.180545e-05  5.784145e-06\n",
       "491  5.065328e-03  4.619337e-08  9.635092e-01  7.392908e-05  3.135137e-02\n",
       "492  3.646384e-02  9.606531e-01  2.318081e-09  2.883147e-03  3.232211e-11\n",
       "493  7.427807e-04  7.620057e-06  9.961189e-01  2.710964e-06  3.127969e-03\n",
       "494  1.078909e-06  2.618394e-06  2.894435e-07  9.876021e-07  9.999950e-01\n",
       "495  7.578716e-03  5.161685e-07  5.507310e-08  9.924194e-01  1.274363e-06\n",
       "496  1.759027e-01  4.834744e-01  9.417763e-02  2.837596e-04  2.461615e-01\n",
       "497  5.554980e-06  9.999943e-01  1.626571e-13  1.028339e-07  1.953231e-08\n",
       "498  9.999973e-01  2.627073e-09  7.970892e-13  2.721571e-06  5.618244e-13\n",
       "499  2.377645e-02  4.428895e-06  1.152405e-03  4.446667e-04  9.746221e-01\n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Convert the prediction probability nparray to pandas dataframe to understand its structure\n",
    "df_pred = pd.DataFrame(pred_probability)\n",
    "display(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This section is for saving the results to the CSV file.\n",
    "# \"\"\"\n",
    "## Get the predicted class indices from model prediction result.(we can check it from the above probability dataframe)\n",
    "predicted_class_indices = np.argmax(pred_probability, axis=1)\n",
    "\n",
    "#default labels from Keras data generator(ie. names of sub-directories of training set)\n",
    "keras_labels = (train_generator.class_indices)\n",
    "#get the names of class labels\n",
    "keras_labels_swap = dict((value, key) for key, value in keras_labels.items())\n",
    "class_name = [keras_labels_swap[idx] for idx in predicted_class_indices]\n",
    "\n",
    "## Reading pre-defined labels from mapping.txt, and store it to a dictionary\n",
    "mapping = {}\n",
    "with open(\"./mapping.txt\") as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split(sep=\",\")\n",
    "        mapping[str(key)] = int(val)\n",
    "\n",
    "## Because predicted_class_indices come from Keras (data generator) default labels,\n",
    "## this may not match our pre-defined labels (from mapping.txt).\n",
    "## I use pandas.Series.map(arg=Dict) to remap predicted_class_indices to pre-defined labels.\n",
    "ps = pd.Series(data = class_name)\n",
    "class_predictions = ps.map(mapping)\n",
    "\n",
    "## Get filenames of all test images\n",
    "files = test_generator.filenames #!this output will include the directory path name!\n",
    "#use string.strip() to retrieve exact filename(without directory path name) of test images\n",
    "filenames = []\n",
    "for num in range(len(files)):\n",
    "    lst = files[num].lstrip(\"testimg/\").rstrip(\".jpg\")\n",
    "    filenames.append(lst)\n",
    "\n",
    "## Save the results to the csv file\n",
    "results = pd.DataFrame({\"id\" : filenames,\n",
    "                        \"class_name\" : class_name,\n",
    "                        \"class\" : class_predictions})\n",
    "results.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "submission = pd.DataFrame({\"id\" : filenames,\n",
    "                           \"class\" : class_predictions})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook FlowerClassification_CNN-TL.ipynb to script\n",
      "[NbConvertApp] Writing 9676 bytes to FlowerClassification_CNN-TL.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script FlowerClassification_CNN-TL.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Experiment Results:\n",
    "# Transfer Learning:\n",
    "# DenseNet121---\n",
    "# 1. Freeze layers of base model before model constructed and model.compile using:\n",
    "#    conv_base.trainable = False \n",
    "#    or\n",
    "#    for layer in conv_base.layers:\n",
    "#         layer.trainable = False\n",
    "# -> val_acc = 0.86\n",
    "# 2. Freeze base model and layers before model constructed and model.compile using:\n",
    "#     conv_base.trainable = False\n",
    "#     for layer in conv_base.layers:\n",
    "#         layer.trainable = False\n",
    "# -> val_acc = 0.85677\n",
    "# 3. Freeze layers after model constructed before model.compile using:\n",
    "#     for layer in conv_base.layers:\n",
    "#         layer.trainable = False\n",
    "# -> val_acc = 0.94\n",
    "# When turning off earlystop, at epoch:70~75, val_acc:0.94 at most.\n",
    "#\n",
    "# Conclusion: Freeze all layers of base model after model constructed before model.compile will get better accuracy.\n",
    "# ?Problem: Why setting \"base_model.trainable = False\" not work well? seems not broadcast to whole model?\n",
    "# \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
