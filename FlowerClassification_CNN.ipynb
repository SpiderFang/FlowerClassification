{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Transfer Learning technique to do images classification ##\n",
    "# \"\"\"\n",
    "# Workaround Notes:\n",
    "# 1. Q: the validation_split argument in ImageDataGenerator not supported in Keras 2.1.3(server version)\n",
    "#    A: upgrade to the latest Keras(version 2.2.2): pip install keras --upgrade\n",
    "# 2. Q: Activation \"softmax\" in the latest Keras(version 2.2.2) not matched TensorFlow 1.4(server version)\n",
    "#    A: change Activation \"softmax\" to tf.nn.softmax\n",
    "# => Keras 2.1.5 is exactly for tensorflow 1.4.1! Instead of using \"pip install keras==2.1.5\" to overcome both Q1&Q2.\n",
    "#\n",
    "# Experimental Result:\n",
    "# Keras 2.1.5 + tensorflow 1.4.1 got better accuracy than Keras 2.2.2 + tensorflow 1.4.1\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 224, 224, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               51380480  \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 51,448,741\n",
      "Trainable params: 51,448,037\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "\n",
    "TRAIN_IMG_DIR = \"./train/\" #training_set at ./train/\n",
    "TEST_IMG_DIR = \"./test/\" #testing_set at ./test/testimg/\n",
    "\n",
    "NUM_CLASSES = 5 #target labels(ground truth), total 5 classes(check mapping.txt)\n",
    "\n",
    "# Image shapes\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "INPUT_SHAPE = (IMG_WIDTH, IMG_HEIGHT, CHANNELS)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "\n",
    "## Build model: (Convolution layer + MaxPooling layer)s + Fully-connected NN layers\n",
    "model = Sequential()\n",
    "#Convolution layer*2 + BN + MaxPooling layer\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=INPUT_SHAPE)) #input_shape argument must be assigned in first layer\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Convolution layer*2 + BN + MaxPooling layer\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Fully-connected NN layers\n",
    "#fully-connected 1st layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#fully-connected final layer\n",
    "# model.add(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "# !change Activation from keras to tf.nn.softmax, because TF version too old on Server!\n",
    "model.add(Dense(NUM_CLASSES))\n",
    "import tensorflow as tf\n",
    "model.add(Activation(tf.nn.softmax))\n",
    "\n",
    "# opt_adam = optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "# opt_rmsprop = optimizers.RMSprop(lr=1e-5, decay=0.01)\n",
    "model.compile(loss = \"categorical_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3062 images belonging to 5 classes.\n",
      "Found 761 images belonging to 5 classes.\n",
      "Found 500 images belonging to 1 classes.\n",
      "3062\n",
      "761\n",
      "500\n",
      "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}\n",
      "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "## Using Keras ImageDataGenerator to load images batch and do data augmentation on the fly.\n",
    "#!validation_split argument not supported in Keras 2.1.3(server version)!\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        rotation_range = 20,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        validation_split = 0.20\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "#         rotation_range = 20,\n",
    "#         width_shift_range = 0.2,\n",
    "#         height_shift_range = 0.2,\n",
    "#         shear_range = 0.2,\n",
    "#         zoom_range = 0.2,\n",
    "#         horizontal_flip = True,\n",
    "        validation_split = 0.20\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "## Using Keras datagen.flow_from_directory to load images from every sub-directories at train,(validation),test directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        directory = TRAIN_IMG_DIR,\n",
    "        target_size = (IMG_WIDTH, IMG_HEIGHT),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = BATCH_SIZE,\n",
    "        class_mode = \"categorical\",\n",
    "        shuffle = True,\n",
    "        seed = 33,\n",
    "        subset = \"training\"\n",
    ")\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        directory = TRAIN_IMG_DIR,\n",
    "        target_size = (IMG_WIDTH, IMG_HEIGHT),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = BATCH_SIZE,\n",
    "        class_mode = \"categorical\",\n",
    "        shuffle = True,\n",
    "        seed = 33,\n",
    "        subset = \"validation\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        directory = TEST_IMG_DIR,\n",
    "        target_size = (IMG_WIDTH, IMG_HEIGHT),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = 1,\n",
    "        class_mode = None,\n",
    "        shuffle = False\n",
    ")\n",
    "\n",
    "## Amounts of individual set: training, validation, test\n",
    "print (train_generator.n) #amounts of train_generator\n",
    "print (validation_generator.n) #amounts of validation_generator\n",
    "print (test_generator.n) #amounts of test_generator\n",
    "\n",
    "## Labels from Keras data generator\n",
    "print (train_generator.class_indices)\n",
    "print (validation_generator.class_indices)\n",
    "\n",
    "## Image shape check\n",
    "print (train_generator.image_shape)\n",
    "print (validation_generator.image_shape)\n",
    "print (test_generator.image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "191/191 [==============================] - 72s 375ms/step - loss: 1.8289 - acc: 0.4059 - val_loss: 1.3398 - val_acc: 0.5059\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50591, saving model to ./checkpoint-01-1.34-0.51.hdf5\n",
      "Epoch 2/100\n",
      "191/191 [==============================] - 37s 193ms/step - loss: 1.4308 - acc: 0.4614 - val_loss: 1.1937 - val_acc: 0.5230\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.50591 to 0.52300, saving model to ./checkpoint-02-1.19-0.52.hdf5\n",
      "Epoch 3/100\n",
      "191/191 [==============================] - 37s 194ms/step - loss: 1.2599 - acc: 0.5074 - val_loss: 1.3160 - val_acc: 0.4717\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.52300\n",
      "Epoch 4/100\n",
      "191/191 [==============================] - 37s 193ms/step - loss: 1.1911 - acc: 0.5495 - val_loss: 1.1534 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.52300 to 0.56636, saving model to ./checkpoint-04-1.15-0.57.hdf5\n",
      "Epoch 5/100\n",
      "191/191 [==============================] - 37s 192ms/step - loss: 1.0999 - acc: 0.5739 - val_loss: 0.8904 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.56636 to 0.65309, saving model to ./checkpoint-05-0.89-0.65.hdf5\n",
      "Epoch 6/100\n",
      "191/191 [==============================] - 37s 195ms/step - loss: 1.0396 - acc: 0.5956 - val_loss: 1.0806 - val_acc: 0.6137\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.65309\n",
      "Epoch 7/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 1.0105 - acc: 0.6137 - val_loss: 0.9224 - val_acc: 0.6570\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.65309 to 0.65703, saving model to ./checkpoint-07-0.92-0.66.hdf5\n",
      "Epoch 8/100\n",
      "191/191 [==============================] - 37s 194ms/step - loss: 1.0054 - acc: 0.6072 - val_loss: 1.1285 - val_acc: 0.5388\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.65703\n",
      "Epoch 9/100\n",
      "191/191 [==============================] - 36s 190ms/step - loss: 1.0136 - acc: 0.6005 - val_loss: 0.9204 - val_acc: 0.6623\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.65703 to 0.66229, saving model to ./checkpoint-09-0.92-0.66.hdf5\n",
      "Epoch 10/100\n",
      "191/191 [==============================] - 36s 191ms/step - loss: 0.9436 - acc: 0.6356 - val_loss: 0.9255 - val_acc: 0.6583\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.66229\n",
      "Epoch 11/100\n",
      "191/191 [==============================] - 36s 189ms/step - loss: 0.9417 - acc: 0.6445 - val_loss: 1.0010 - val_acc: 0.6084\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.66229\n",
      "Epoch 12/100\n",
      "191/191 [==============================] - 36s 186ms/step - loss: 0.9040 - acc: 0.6455 - val_loss: 0.9200 - val_acc: 0.6347\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.66229\n",
      "Epoch 13/100\n",
      "191/191 [==============================] - 36s 187ms/step - loss: 0.8922 - acc: 0.6516 - val_loss: 0.8087 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.66229 to 0.70828, saving model to ./checkpoint-13-0.81-0.71.hdf5\n",
      "Epoch 14/100\n",
      "191/191 [==============================] - 36s 188ms/step - loss: 0.8606 - acc: 0.6738 - val_loss: 0.8446 - val_acc: 0.6965\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.70828\n",
      "Epoch 15/100\n",
      "191/191 [==============================] - 35s 186ms/step - loss: 0.8384 - acc: 0.6802 - val_loss: 0.8918 - val_acc: 0.6321\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.70828\n",
      "Epoch 16/100\n",
      "191/191 [==============================] - 35s 186ms/step - loss: 0.8481 - acc: 0.6796 - val_loss: 0.7922 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.70828\n",
      "Epoch 17/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.7898 - acc: 0.6982 - val_loss: 0.6847 - val_acc: 0.7556\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.70828 to 0.75558, saving model to ./checkpoint-17-0.68-0.76.hdf5\n",
      "Epoch 18/100\n",
      "191/191 [==============================] - 36s 187ms/step - loss: 0.8048 - acc: 0.6855 - val_loss: 0.8969 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.75558\n",
      "Epoch 19/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.7551 - acc: 0.7133 - val_loss: 0.9425 - val_acc: 0.6189\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.75558\n",
      "Epoch 20/100\n",
      "191/191 [==============================] - 35s 185ms/step - loss: 0.7655 - acc: 0.7051 - val_loss: 1.0091 - val_acc: 0.6399\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.75558\n",
      "Epoch 21/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.7312 - acc: 0.7148 - val_loss: 0.7341 - val_acc: 0.7424\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.75558\n",
      "Epoch 22/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.7386 - acc: 0.7120 - val_loss: 0.7926 - val_acc: 0.7004\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.75558\n",
      "Epoch 23/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.7416 - acc: 0.7200 - val_loss: 0.7557 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.75558\n",
      "Epoch 24/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.7186 - acc: 0.7234 - val_loss: 0.7609 - val_acc: 0.7227\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.75558\n",
      "Epoch 25/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.6883 - acc: 0.7426 - val_loss: 0.8377 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.75558\n",
      "Epoch 26/100\n",
      "191/191 [==============================] - 35s 185ms/step - loss: 0.6663 - acc: 0.7582 - val_loss: 0.7317 - val_acc: 0.7346\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.75558\n",
      "Epoch 27/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.6782 - acc: 0.7557 - val_loss: 0.6522 - val_acc: 0.7648\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.75558 to 0.76478, saving model to ./checkpoint-27-0.65-0.76.hdf5\n",
      "Epoch 28/100\n",
      "191/191 [==============================] - 36s 186ms/step - loss: 0.6880 - acc: 0.7368 - val_loss: 0.7939 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76478\n",
      "Epoch 29/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.6559 - acc: 0.7574 - val_loss: 0.7112 - val_acc: 0.7530\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76478\n",
      "Epoch 30/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.6553 - acc: 0.7481 - val_loss: 0.9350 - val_acc: 0.6859\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76478\n",
      "Epoch 31/100\n",
      "191/191 [==============================] - 35s 185ms/step - loss: 0.6689 - acc: 0.7415 - val_loss: 0.9064 - val_acc: 0.6991\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76478\n",
      "Epoch 32/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.6677 - acc: 0.7503 - val_loss: 0.6225 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76478\n",
      "Epoch 33/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.6357 - acc: 0.7598 - val_loss: 0.7501 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76478\n",
      "Epoch 34/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.6340 - acc: 0.7655 - val_loss: 0.5774 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.76478 to 0.78318, saving model to ./checkpoint-34-0.58-0.78.hdf5\n",
      "Epoch 35/100\n",
      "191/191 [==============================] - 36s 186ms/step - loss: 0.6218 - acc: 0.7694 - val_loss: 0.6824 - val_acc: 0.7477\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.78318\n",
      "Epoch 36/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.6031 - acc: 0.7735 - val_loss: 0.7586 - val_acc: 0.7057\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.78318\n",
      "Epoch 37/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.6211 - acc: 0.7555 - val_loss: 0.6627 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.78318\n",
      "Epoch 38/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.5875 - acc: 0.7782 - val_loss: 0.5536 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.78318 to 0.79369, saving model to ./checkpoint-38-0.55-0.79.hdf5\n",
      "Epoch 39/100\n",
      "191/191 [==============================] - 36s 186ms/step - loss: 0.5921 - acc: 0.7804 - val_loss: 0.6848 - val_acc: 0.7595\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.79369\n",
      "Epoch 40/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.5711 - acc: 0.7826 - val_loss: 0.8881 - val_acc: 0.7411\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.79369\n",
      "Epoch 41/100\n",
      "191/191 [==============================] - 35s 185ms/step - loss: 0.5753 - acc: 0.7916 - val_loss: 0.5709 - val_acc: 0.8016\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.79369 to 0.80158, saving model to ./checkpoint-41-0.57-0.80.hdf5\n",
      "Epoch 42/100\n",
      "191/191 [==============================] - 35s 186ms/step - loss: 0.5854 - acc: 0.7888 - val_loss: 0.6009 - val_acc: 0.7819\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.80158\n",
      "Epoch 43/100\n",
      "191/191 [==============================] - 36s 187ms/step - loss: 0.5899 - acc: 0.7838 - val_loss: 0.8029 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.80158\n",
      "Epoch 44/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.5334 - acc: 0.8046 - val_loss: 0.5941 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.80158 to 0.80420, saving model to ./checkpoint-44-0.59-0.80.hdf5\n",
      "Epoch 45/100\n",
      "191/191 [==============================] - 36s 186ms/step - loss: 0.5829 - acc: 0.7889 - val_loss: 0.5754 - val_acc: 0.7989\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.80420\n",
      "Epoch 46/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.5401 - acc: 0.7945 - val_loss: 0.6240 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.80420 to 0.80946, saving model to ./checkpoint-46-0.62-0.81.hdf5\n",
      "Epoch 47/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.5628 - acc: 0.7975 - val_loss: 0.6143 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.80946\n",
      "Epoch 48/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.5254 - acc: 0.8109 - val_loss: 0.7193 - val_acc: 0.7687\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.80946\n",
      "Epoch 49/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.5576 - acc: 0.7918 - val_loss: 0.7311 - val_acc: 0.7740\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.80946\n",
      "Epoch 50/100\n",
      "191/191 [==============================] - 35s 185ms/step - loss: 0.5175 - acc: 0.8096 - val_loss: 0.6681 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.80946\n",
      "Epoch 51/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.5574 - acc: 0.7885 - val_loss: 0.7078 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.80946\n",
      "Epoch 52/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.4826 - acc: 0.8210 - val_loss: 0.5872 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00052: val_acc improved from 0.80946 to 0.81340, saving model to ./checkpoint-52-0.59-0.81.hdf5\n",
      "Epoch 53/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.4980 - acc: 0.8198 - val_loss: 0.6543 - val_acc: 0.7845\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.81340\n",
      "Epoch 54/100\n",
      "191/191 [==============================] - 35s 185ms/step - loss: 0.5292 - acc: 0.8043 - val_loss: 0.6010 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.81340\n",
      "Epoch 55/100\n",
      "191/191 [==============================] - 35s 185ms/step - loss: 0.5202 - acc: 0.8068 - val_loss: 0.9044 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.81340\n",
      "Epoch 56/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.6588 - acc: 0.7592 - val_loss: 0.9798 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.81340\n",
      "Epoch 57/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.6068 - acc: 0.7764 - val_loss: 0.6948 - val_acc: 0.7727\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.81340\n",
      "Epoch 58/100\n",
      "191/191 [==============================] - 35s 185ms/step - loss: 0.5472 - acc: 0.8102 - val_loss: 0.5676 - val_acc: 0.7989\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.81340\n",
      "Epoch 59/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.5240 - acc: 0.8135 - val_loss: 0.5756 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.81340\n",
      "Epoch 60/100\n",
      "191/191 [==============================] - 35s 181ms/step - loss: 0.4816 - acc: 0.8209 - val_loss: 0.6331 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.81340\n",
      "Epoch 61/100\n",
      "191/191 [==============================] - 35s 185ms/step - loss: 0.4694 - acc: 0.8278 - val_loss: 0.5140 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.81340 to 0.81603, saving model to ./checkpoint-61-0.51-0.82.hdf5\n",
      "Epoch 62/100\n",
      "191/191 [==============================] - 36s 186ms/step - loss: 0.5167 - acc: 0.8236 - val_loss: 0.5838 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.81603\n",
      "Epoch 63/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.4650 - acc: 0.8255 - val_loss: 0.6070 - val_acc: 0.8081\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.81603\n",
      "Epoch 64/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.4816 - acc: 0.8281 - val_loss: 0.5875 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.81603\n",
      "Epoch 65/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.5099 - acc: 0.8154 - val_loss: 0.9234 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.81603\n",
      "Epoch 66/100\n",
      "191/191 [==============================] - 35s 185ms/step - loss: 0.4944 - acc: 0.8134 - val_loss: 0.8051 - val_acc: 0.7766\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.81603\n",
      "Epoch 67/100\n",
      "191/191 [==============================] - 35s 183ms/step - loss: 0.5214 - acc: 0.8112 - val_loss: 0.5985 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.81603\n",
      "Epoch 68/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.4637 - acc: 0.8362 - val_loss: 0.6355 - val_acc: 0.8108\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.81603\n",
      "Epoch 69/100\n",
      "191/191 [==============================] - 35s 182ms/step - loss: 0.4830 - acc: 0.8190 - val_loss: 0.5912 - val_acc: 0.8016\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.81603\n",
      "Epoch 70/100\n",
      "191/191 [==============================] - 35s 184ms/step - loss: 0.4494 - acc: 0.8408 - val_loss: 0.9373 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.81603\n",
      "Epoch 71/100\n",
      "191/191 [==============================] - 35s 186ms/step - loss: 0.4261 - acc: 0.8475 - val_loss: 0.6968 - val_acc: 0.7845\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.81603\n",
      "Epoch 00071: early stopping\n",
      "500/500 [==============================] - 6s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "## Fitting/Training the model\n",
    "STEPS_PER_EPOCH = train_generator.n // BATCH_SIZE\n",
    "VALIDATION_STEPS = validation_generator.n // BATCH_SIZE\n",
    "\n",
    "# Callbacks setting\n",
    "FILE_PATH = \"./checkpoint-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}.hdf5\"\n",
    "EarlyStop = EarlyStopping(monitor=\"val_acc\", patience=10, verbose=1, mode=\"max\")\n",
    "Checkpoint = ModelCheckpoint(FILE_PATH, monitor=\"val_acc\", verbose=1, save_best_only=True, mode=\"max\")\n",
    "Callback_list = [EarlyStop, Checkpoint]\n",
    "\n",
    "history = model.fit_generator(\n",
    "                generator = train_generator,\n",
    "                steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                epochs = EPOCHS,\n",
    "                callbacks = Callback_list,\n",
    "                validation_data = validation_generator,\n",
    "                validation_steps = VALIDATION_STEPS,\n",
    "                shuffle = True\n",
    ")\n",
    "\n",
    "## Evaluate the model\n",
    "# model.evaluate_generator(generator = )\n",
    "\n",
    "## Predict the test set, then we'll get a probability nparray\n",
    "test_generator.reset()\n",
    "pred_probability = model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.740959e-01</td>\n",
       "      <td>1.035689e-01</td>\n",
       "      <td>7.262916e-03</td>\n",
       "      <td>1.341955e-02</td>\n",
       "      <td>1.652773e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.929080e-03</td>\n",
       "      <td>7.174431e-04</td>\n",
       "      <td>3.617489e-04</td>\n",
       "      <td>9.621175e-01</td>\n",
       "      <td>3.287419e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.161377e-03</td>\n",
       "      <td>1.395368e-04</td>\n",
       "      <td>4.412198e-02</td>\n",
       "      <td>3.368535e-06</td>\n",
       "      <td>9.475738e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.799526e-04</td>\n",
       "      <td>5.449135e-05</td>\n",
       "      <td>2.683922e-05</td>\n",
       "      <td>9.993231e-01</td>\n",
       "      <td>1.565954e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.811743e-03</td>\n",
       "      <td>3.579692e-06</td>\n",
       "      <td>1.047605e-07</td>\n",
       "      <td>9.951839e-01</td>\n",
       "      <td>7.413789e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.823314e-03</td>\n",
       "      <td>1.529317e-05</td>\n",
       "      <td>9.540991e-01</td>\n",
       "      <td>4.943111e-05</td>\n",
       "      <td>4.201282e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.999734e-01</td>\n",
       "      <td>2.521053e-08</td>\n",
       "      <td>8.549175e-06</td>\n",
       "      <td>2.187151e-09</td>\n",
       "      <td>1.797246e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.110406e-01</td>\n",
       "      <td>2.661431e-04</td>\n",
       "      <td>8.776755e-01</td>\n",
       "      <td>1.188366e-03</td>\n",
       "      <td>9.829368e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.409052e-03</td>\n",
       "      <td>2.248494e-03</td>\n",
       "      <td>8.488712e-03</td>\n",
       "      <td>9.329426e-01</td>\n",
       "      <td>5.391110e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.258468e-02</td>\n",
       "      <td>1.749023e-03</td>\n",
       "      <td>9.148928e-03</td>\n",
       "      <td>9.471747e-01</td>\n",
       "      <td>2.934270e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.802553e-08</td>\n",
       "      <td>9.924815e-01</td>\n",
       "      <td>6.219780e-07</td>\n",
       "      <td>1.217957e-11</td>\n",
       "      <td>7.518020e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>5.821250e-10</td>\n",
       "      <td>3.207056e-08</td>\n",
       "      <td>1.810045e-10</td>\n",
       "      <td>2.278771e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.437660e-02</td>\n",
       "      <td>9.725936e-03</td>\n",
       "      <td>1.137684e-02</td>\n",
       "      <td>1.578074e-01</td>\n",
       "      <td>7.767133e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.561873e-01</td>\n",
       "      <td>4.062696e-01</td>\n",
       "      <td>4.939991e-03</td>\n",
       "      <td>7.513818e-03</td>\n",
       "      <td>2.508924e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.523954e-05</td>\n",
       "      <td>7.892330e-01</td>\n",
       "      <td>9.684544e-03</td>\n",
       "      <td>3.767642e-07</td>\n",
       "      <td>2.010168e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.972227e-03</td>\n",
       "      <td>3.135131e-04</td>\n",
       "      <td>9.888452e-01</td>\n",
       "      <td>9.872496e-07</td>\n",
       "      <td>2.868119e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.331106e-02</td>\n",
       "      <td>2.197014e-07</td>\n",
       "      <td>3.114170e-04</td>\n",
       "      <td>1.170611e-07</td>\n",
       "      <td>9.863772e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.440195e-04</td>\n",
       "      <td>1.005668e-03</td>\n",
       "      <td>1.678847e-02</td>\n",
       "      <td>3.394674e-04</td>\n",
       "      <td>9.813224e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.098002e-01</td>\n",
       "      <td>2.206654e-06</td>\n",
       "      <td>7.386431e-13</td>\n",
       "      <td>8.901976e-01</td>\n",
       "      <td>6.894119e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.539631e-04</td>\n",
       "      <td>7.890273e-01</td>\n",
       "      <td>2.378514e-03</td>\n",
       "      <td>1.426662e-06</td>\n",
       "      <td>2.078387e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.690103e-04</td>\n",
       "      <td>5.028243e-06</td>\n",
       "      <td>9.808055e-01</td>\n",
       "      <td>2.088260e-04</td>\n",
       "      <td>1.871156e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.165686e-10</td>\n",
       "      <td>9.258059e-09</td>\n",
       "      <td>9.999970e-01</td>\n",
       "      <td>7.031988e-10</td>\n",
       "      <td>2.961000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.842784e-02</td>\n",
       "      <td>2.057721e-04</td>\n",
       "      <td>5.980133e-01</td>\n",
       "      <td>3.561283e-05</td>\n",
       "      <td>3.733174e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.629917e-04</td>\n",
       "      <td>4.605403e-04</td>\n",
       "      <td>5.052357e-03</td>\n",
       "      <td>2.220768e-05</td>\n",
       "      <td>9.939020e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.008630e-05</td>\n",
       "      <td>4.604156e-04</td>\n",
       "      <td>1.440872e-04</td>\n",
       "      <td>1.797915e-07</td>\n",
       "      <td>9.993253e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.276092e-04</td>\n",
       "      <td>1.179050e-03</td>\n",
       "      <td>3.498256e-03</td>\n",
       "      <td>1.694059e-06</td>\n",
       "      <td>9.951933e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.591922e-04</td>\n",
       "      <td>3.110371e-06</td>\n",
       "      <td>4.554440e-03</td>\n",
       "      <td>2.866209e-06</td>\n",
       "      <td>9.952803e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.936387e-04</td>\n",
       "      <td>1.121385e-02</td>\n",
       "      <td>6.499761e-02</td>\n",
       "      <td>1.905707e-05</td>\n",
       "      <td>9.232758e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.673746e-04</td>\n",
       "      <td>1.493862e-04</td>\n",
       "      <td>1.089523e-03</td>\n",
       "      <td>7.280439e-07</td>\n",
       "      <td>9.983930e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.719407e-04</td>\n",
       "      <td>6.606122e-04</td>\n",
       "      <td>2.665950e-02</td>\n",
       "      <td>7.761932e-06</td>\n",
       "      <td>9.722002e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>3.586924e-07</td>\n",
       "      <td>1.637396e-08</td>\n",
       "      <td>2.760232e-06</td>\n",
       "      <td>1.069064e-10</td>\n",
       "      <td>9.999969e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>9.999225e-01</td>\n",
       "      <td>1.027639e-08</td>\n",
       "      <td>1.022045e-05</td>\n",
       "      <td>6.497162e-07</td>\n",
       "      <td>6.658944e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1.760112e-03</td>\n",
       "      <td>1.421472e-04</td>\n",
       "      <td>3.341688e-03</td>\n",
       "      <td>8.067704e-01</td>\n",
       "      <td>1.879857e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>3.830132e-05</td>\n",
       "      <td>9.882560e-01</td>\n",
       "      <td>1.646602e-03</td>\n",
       "      <td>1.314844e-05</td>\n",
       "      <td>1.004596e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>9.981000e-01</td>\n",
       "      <td>1.097109e-03</td>\n",
       "      <td>2.030213e-04</td>\n",
       "      <td>4.860536e-06</td>\n",
       "      <td>5.950728e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1.118409e-01</td>\n",
       "      <td>4.281206e-03</td>\n",
       "      <td>1.592671e-03</td>\n",
       "      <td>5.336616e-01</td>\n",
       "      <td>3.486236e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>1.509821e-03</td>\n",
       "      <td>5.275873e-05</td>\n",
       "      <td>6.498821e-04</td>\n",
       "      <td>9.800895e-01</td>\n",
       "      <td>1.769809e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>5.063592e-02</td>\n",
       "      <td>8.718254e-04</td>\n",
       "      <td>1.085305e-03</td>\n",
       "      <td>9.100681e-01</td>\n",
       "      <td>3.733884e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1.428040e-01</td>\n",
       "      <td>6.295734e-04</td>\n",
       "      <td>5.881372e-04</td>\n",
       "      <td>7.980961e-01</td>\n",
       "      <td>5.788213e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>1.465392e-01</td>\n",
       "      <td>8.158953e-03</td>\n",
       "      <td>9.196418e-04</td>\n",
       "      <td>8.260430e-01</td>\n",
       "      <td>1.833929e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>2.495324e-02</td>\n",
       "      <td>1.232523e-02</td>\n",
       "      <td>7.159092e-04</td>\n",
       "      <td>9.559587e-01</td>\n",
       "      <td>6.047003e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>1.406970e-01</td>\n",
       "      <td>2.054922e-02</td>\n",
       "      <td>4.600201e-01</td>\n",
       "      <td>9.212176e-04</td>\n",
       "      <td>3.778124e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>1.608083e-01</td>\n",
       "      <td>6.676732e-01</td>\n",
       "      <td>7.757724e-02</td>\n",
       "      <td>4.828820e-02</td>\n",
       "      <td>4.565301e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.677997e-02</td>\n",
       "      <td>8.946608e-05</td>\n",
       "      <td>9.391402e-05</td>\n",
       "      <td>9.725309e-01</td>\n",
       "      <td>5.058044e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>6.659159e-03</td>\n",
       "      <td>3.106535e-07</td>\n",
       "      <td>1.620155e-08</td>\n",
       "      <td>9.933400e-01</td>\n",
       "      <td>5.632886e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>2.648291e-02</td>\n",
       "      <td>9.491782e-05</td>\n",
       "      <td>1.430832e-04</td>\n",
       "      <td>9.324331e-01</td>\n",
       "      <td>4.084598e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1.425973e-02</td>\n",
       "      <td>1.010268e-03</td>\n",
       "      <td>5.466365e-01</td>\n",
       "      <td>8.212674e-06</td>\n",
       "      <td>4.380853e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>7.951947e-03</td>\n",
       "      <td>6.228734e-04</td>\n",
       "      <td>1.644452e-05</td>\n",
       "      <td>9.858378e-01</td>\n",
       "      <td>5.570996e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>3.302549e-01</td>\n",
       "      <td>7.460364e-04</td>\n",
       "      <td>6.357017e-01</td>\n",
       "      <td>3.106938e-03</td>\n",
       "      <td>3.019037e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>2.276804e-04</td>\n",
       "      <td>6.034023e-07</td>\n",
       "      <td>9.907039e-01</td>\n",
       "      <td>1.930836e-06</td>\n",
       "      <td>9.065872e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>9.725072e-01</td>\n",
       "      <td>2.616004e-04</td>\n",
       "      <td>6.373118e-04</td>\n",
       "      <td>9.691093e-03</td>\n",
       "      <td>1.690274e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>2.922229e-01</td>\n",
       "      <td>5.807583e-05</td>\n",
       "      <td>5.221089e-01</td>\n",
       "      <td>9.986759e-06</td>\n",
       "      <td>1.856002e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>2.007977e-03</td>\n",
       "      <td>9.778774e-01</td>\n",
       "      <td>5.123144e-03</td>\n",
       "      <td>5.723653e-06</td>\n",
       "      <td>1.498579e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>3.649479e-02</td>\n",
       "      <td>9.183020e-05</td>\n",
       "      <td>9.460275e-01</td>\n",
       "      <td>6.459691e-06</td>\n",
       "      <td>1.737945e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>4.749634e-05</td>\n",
       "      <td>2.174114e-08</td>\n",
       "      <td>5.537936e-06</td>\n",
       "      <td>3.577647e-05</td>\n",
       "      <td>9.999112e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>7.040798e-03</td>\n",
       "      <td>1.540349e-05</td>\n",
       "      <td>1.147295e-04</td>\n",
       "      <td>9.922264e-01</td>\n",
       "      <td>6.028005e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>3.444007e-01</td>\n",
       "      <td>2.850723e-02</td>\n",
       "      <td>1.285797e-02</td>\n",
       "      <td>4.221556e-02</td>\n",
       "      <td>5.720186e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1.216574e-02</td>\n",
       "      <td>8.408038e-01</td>\n",
       "      <td>1.000922e-01</td>\n",
       "      <td>2.307756e-03</td>\n",
       "      <td>4.463057e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>9.999354e-01</td>\n",
       "      <td>1.049995e-05</td>\n",
       "      <td>1.660642e-06</td>\n",
       "      <td>8.078330e-09</td>\n",
       "      <td>5.250318e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1.095743e-01</td>\n",
       "      <td>2.583893e-02</td>\n",
       "      <td>4.637397e-01</td>\n",
       "      <td>2.883418e-02</td>\n",
       "      <td>3.720129e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4\n",
       "0    8.740959e-01  1.035689e-01  7.262916e-03  1.341955e-02  1.652773e-03\n",
       "1    3.929080e-03  7.174431e-04  3.617489e-04  9.621175e-01  3.287419e-02\n",
       "2    8.161377e-03  1.395368e-04  4.412198e-02  3.368535e-06  9.475738e-01\n",
       "3    5.799526e-04  5.449135e-05  2.683922e-05  9.993231e-01  1.565954e-05\n",
       "4    4.811743e-03  3.579692e-06  1.047605e-07  9.951839e-01  7.413789e-07\n",
       "5    3.823314e-03  1.529317e-05  9.540991e-01  4.943111e-05  4.201282e-02\n",
       "6    9.999734e-01  2.521053e-08  8.549175e-06  2.187151e-09  1.797246e-05\n",
       "7    1.110406e-01  2.661431e-04  8.776755e-01  1.188366e-03  9.829368e-03\n",
       "8    2.409052e-03  2.248494e-03  8.488712e-03  9.329426e-01  5.391110e-02\n",
       "9    1.258468e-02  1.749023e-03  9.148928e-03  9.471747e-01  2.934270e-02\n",
       "10   3.802553e-08  9.924815e-01  6.219780e-07  1.217957e-11  7.518020e-03\n",
       "11   9.999998e-01  5.821250e-10  3.207056e-08  1.810045e-10  2.278771e-07\n",
       "12   4.437660e-02  9.725936e-03  1.137684e-02  1.578074e-01  7.767133e-01\n",
       "13   5.561873e-01  4.062696e-01  4.939991e-03  7.513818e-03  2.508924e-02\n",
       "14   6.523954e-05  7.892330e-01  9.684544e-03  3.767642e-07  2.010168e-01\n",
       "15   7.972227e-03  3.135131e-04  9.888452e-01  9.872496e-07  2.868119e-03\n",
       "16   1.331106e-02  2.197014e-07  3.114170e-04  1.170611e-07  9.863772e-01\n",
       "17   5.440195e-04  1.005668e-03  1.678847e-02  3.394674e-04  9.813224e-01\n",
       "18   1.098002e-01  2.206654e-06  7.386431e-13  8.901976e-01  6.894119e-10\n",
       "19   7.539631e-04  7.890273e-01  2.378514e-03  1.426662e-06  2.078387e-01\n",
       "20   2.690103e-04  5.028243e-06  9.808055e-01  2.088260e-04  1.871156e-02\n",
       "21   8.165686e-10  9.258059e-09  9.999970e-01  7.031988e-10  2.961000e-06\n",
       "22   2.842784e-02  2.057721e-04  5.980133e-01  3.561283e-05  3.733174e-01\n",
       "23   5.629917e-04  4.605403e-04  5.052357e-03  2.220768e-05  9.939020e-01\n",
       "24   7.008630e-05  4.604156e-04  1.440872e-04  1.797915e-07  9.993253e-01\n",
       "25   1.276092e-04  1.179050e-03  3.498256e-03  1.694059e-06  9.951933e-01\n",
       "26   1.591922e-04  3.110371e-06  4.554440e-03  2.866209e-06  9.952803e-01\n",
       "27   4.936387e-04  1.121385e-02  6.499761e-02  1.905707e-05  9.232758e-01\n",
       "28   3.673746e-04  1.493862e-04  1.089523e-03  7.280439e-07  9.983930e-01\n",
       "29   4.719407e-04  6.606122e-04  2.665950e-02  7.761932e-06  9.722002e-01\n",
       "..            ...           ...           ...           ...           ...\n",
       "470  3.586924e-07  1.637396e-08  2.760232e-06  1.069064e-10  9.999969e-01\n",
       "471  9.999225e-01  1.027639e-08  1.022045e-05  6.497162e-07  6.658944e-05\n",
       "472  1.760112e-03  1.421472e-04  3.341688e-03  8.067704e-01  1.879857e-01\n",
       "473  3.830132e-05  9.882560e-01  1.646602e-03  1.314844e-05  1.004596e-02\n",
       "474  9.981000e-01  1.097109e-03  2.030213e-04  4.860536e-06  5.950728e-04\n",
       "475  1.118409e-01  4.281206e-03  1.592671e-03  5.336616e-01  3.486236e-01\n",
       "476  1.509821e-03  5.275873e-05  6.498821e-04  9.800895e-01  1.769809e-02\n",
       "477  5.063592e-02  8.718254e-04  1.085305e-03  9.100681e-01  3.733884e-02\n",
       "478  1.428040e-01  6.295734e-04  5.881372e-04  7.980961e-01  5.788213e-02\n",
       "479  1.465392e-01  8.158953e-03  9.196418e-04  8.260430e-01  1.833929e-02\n",
       "480  2.495324e-02  1.232523e-02  7.159092e-04  9.559587e-01  6.047003e-03\n",
       "481  1.406970e-01  2.054922e-02  4.600201e-01  9.212176e-04  3.778124e-01\n",
       "482  1.608083e-01  6.676732e-01  7.757724e-02  4.828820e-02  4.565301e-02\n",
       "483  2.677997e-02  8.946608e-05  9.391402e-05  9.725309e-01  5.058044e-04\n",
       "484  6.659159e-03  3.106535e-07  1.620155e-08  9.933400e-01  5.632886e-07\n",
       "485  2.648291e-02  9.491782e-05  1.430832e-04  9.324331e-01  4.084598e-02\n",
       "486  1.425973e-02  1.010268e-03  5.466365e-01  8.212674e-06  4.380853e-01\n",
       "487  7.951947e-03  6.228734e-04  1.644452e-05  9.858378e-01  5.570996e-03\n",
       "488  3.302549e-01  7.460364e-04  6.357017e-01  3.106938e-03  3.019037e-02\n",
       "489  2.276804e-04  6.034023e-07  9.907039e-01  1.930836e-06  9.065872e-03\n",
       "490  9.725072e-01  2.616004e-04  6.373118e-04  9.691093e-03  1.690274e-02\n",
       "491  2.922229e-01  5.807583e-05  5.221089e-01  9.986759e-06  1.856002e-01\n",
       "492  2.007977e-03  9.778774e-01  5.123144e-03  5.723653e-06  1.498579e-02\n",
       "493  3.649479e-02  9.183020e-05  9.460275e-01  6.459691e-06  1.737945e-02\n",
       "494  4.749634e-05  2.174114e-08  5.537936e-06  3.577647e-05  9.999112e-01\n",
       "495  7.040798e-03  1.540349e-05  1.147295e-04  9.922264e-01  6.028005e-04\n",
       "496  3.444007e-01  2.850723e-02  1.285797e-02  4.221556e-02  5.720186e-01\n",
       "497  1.216574e-02  8.408038e-01  1.000922e-01  2.307756e-03  4.463057e-02\n",
       "498  9.999354e-01  1.049995e-05  1.660642e-06  8.078330e-09  5.250318e-05\n",
       "499  1.095743e-01  2.583893e-02  4.637397e-01  2.883418e-02  3.720129e-01\n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Convert the prediction probability nparray to pandas dataframe to understand its structure\n",
    "df_pred = pd.DataFrame(pred_probability)\n",
    "display(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This section is for saving the results to the CSV file.\n",
    "# \"\"\"\n",
    "## Get the predicted class indices from model prediction result.(we can check it from the above probability dataframe)\n",
    "predicted_class_indices = np.argmax(pred_probability, axis=1)\n",
    "\n",
    "#default labels from Keras data generator(ie. names of sub-directories of training set)\n",
    "keras_labels = (train_generator.class_indices)\n",
    "#get the names of class labels\n",
    "keras_labels_swap = dict((value, key) for key, value in keras_labels.items())\n",
    "class_name = [keras_labels_swap[idx] for idx in predicted_class_indices]\n",
    "\n",
    "## Reading pre-defined labels from mapping.txt, and store it to a dictionary\n",
    "mapping = {}\n",
    "with open(\"./mapping.txt\") as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split(sep=\",\")\n",
    "        mapping[str(key)] = int(val)\n",
    "\n",
    "## Because predicted_class_indices come from Keras (data generator) default labels,\n",
    "## this may not match our pre-defined labels (from mapping.txt).\n",
    "## I use pandas.Series.map(arg=Dict) to remap predicted_class_indices to pre-defined labels.\n",
    "ps = pd.Series(data = class_name)\n",
    "class_predictions = ps.map(mapping)\n",
    "\n",
    "## Get filenames of all test images\n",
    "files = test_generator.filenames #!this output will include the directory path name!\n",
    "#use string.strip() to retrieve exact filename(without directory path name) of test images\n",
    "filenames = []\n",
    "for num in range(len(files)):\n",
    "    lst = files[num].lstrip(\"testimg/\").rstrip(\".jpg\")\n",
    "    filenames.append(lst)\n",
    "\n",
    "## Save the results to the csv file\n",
    "results = pd.DataFrame({\"id\" : filenames,\n",
    "                        \"class_name\" : class_name,\n",
    "                        \"class\" : class_predictions})\n",
    "results.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "submission = pd.DataFrame({\"id\" : filenames,\n",
    "                           \"class\" : class_predictions})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook FlowerClassification_CNN.ipynb to script\n",
      "[NbConvertApp] Writing 8125 bytes to FlowerClassification_CNN.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script FlowerClassification_CNN.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
